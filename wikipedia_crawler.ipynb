{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wikipedia_crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7xo+ING5g2rd/k5vOEjy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamahAyman/Geopedia_DatabaseSystems/blob/main/wikipedia_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMv_wq_du0NX",
        "outputId": "a2b3c035-21ab-4a77-bb5b-da27c0bc0e68"
      },
      "source": [
        "#installations\n",
        "!pip install beautifulsoup4\n",
        "!pip install pandas\n",
        "\n",
        "# import the libraries\n",
        "import requests\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4 import NavigableString\n",
        "import re"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhpccEwiWM0l"
      },
      "source": [
        "### Getting Continents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XVZy4a9Wye6"
      },
      "source": [
        "def get_continents():\n",
        "    #the seed URL of all continents of the world\n",
        "    URL = \"https://en.wikipedia.org/wiki/List_of_sovereign_states_and_dependent_territories_in_\"\n",
        "\n",
        "    #list for the continents names and their wiki URLs\n",
        "    continents = {}\n",
        "    continents ['continent_name'] = ['Africa','Asia','Europe','North_America','South_America','Oceania','Antarctica']\n",
        "    continents ['continent_url']  = []\n",
        "    \n",
        "    for i in range(7):\n",
        "        continents ['continent_url'].append(URL+continents ['continent_name'][i])\n",
        "    \n",
        "    return continents"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N86W9u3VVzB-"
      },
      "source": [
        "### Getting Countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7uDswsIkN8v"
      },
      "source": [
        "def get_countries(continents):\n",
        "    #list of all the countries of the world (from all the 7 continents)\n",
        "    countries = {}\n",
        "    countries['country_index'] = []\n",
        "    countries['country_name']  = []\n",
        "    countries['country_url']  = []\n",
        "    countries_counter = 0\n",
        "\n",
        "    #getting the countries names and URLs\n",
        "    for continent in range(7):\n",
        "        page = requests.get(continents ['continent_url'][continent])\n",
        "        soup = BeautifulSoup(page.content)\n",
        "\n",
        "        # getting the table rows\n",
        "        table_rows = list(soup.find(class_ = \"wikitable\").tbody.children) \n",
        "        table_rows_num = len(table_rows)\n",
        "        \n",
        "        #getting the country_name\n",
        "        for i in range(table_rows_num): \n",
        "            table_row = table_rows[i]\n",
        "            country_name = None\n",
        "            if type(table_row)!= bs4.element.NavigableString:           \n",
        "                for table_data in table_row.children:\n",
        "                    # getting country_name\n",
        "                    if type(table_data)!= bs4.element.NavigableString:\n",
        "                        country_tag = table_data.find(\"a\", title=True)\n",
        "                        if country_tag:\n",
        "                            if not country_name:\n",
        "                                country_name = country_tag.text\n",
        "                                if country_name:\n",
        "                                    countries_counter +=1\n",
        "                                    countries['country_name'].append(country_name)\n",
        "                                    countries['country_index'].append(countries_counter)\n",
        "                                    countries['country_url'].append('https://en.wikipedia.org' + country_tag.get('href'))\n",
        "    return countries                          "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2UVHee6kN4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7645d1e-5c7d-4348-84b6-ab1d3728c9c0"
      },
      "source": [
        "############### defining the methods to get president info ###############\n",
        "presidents = {}\n",
        "presidents['name'] = []\n",
        "presidents['birthdate'] = []\n",
        "presidents['assuming_office_date'] = []\n",
        "presidents['political_party'] = []\n",
        "presidents['president_name'] = []\n",
        "presidents['ruling_country'] = []\n",
        "\n",
        "president_entry = []\n",
        "#getting the president name\n",
        "def get_president_name(url):\n",
        "    name=url.rsplit('/',1)\n",
        "    name=url.rsplit('/',1)[1]\n",
        "    name=name.replace('_', ' ')\n",
        "    return name\n",
        "\n",
        "#getting the president birthdate\n",
        "def get_president_birthdate(url):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content)\n",
        "    table = soup.find(class_ = \"infobox vcard\").tbody.children     \n",
        "    if type(table)!= bs4.element.NavigableString:           \n",
        "        for table_data in table:\n",
        "            if type(table_data)!= bs4.element.NavigableString:\n",
        "                tag = table_data.find(\"th\")\n",
        "                if (tag is None):\n",
        "                  continue\n",
        "                if (tag.text == \"Born\"):\n",
        "                  birthdate = tag.find_next_sibling()\n",
        "                  birthdate = birthdate.find(\"span\").text\n",
        "    return birthdate\n",
        "\n",
        "#getting the date the president assumed office\n",
        "def get_president_assuming_office_date(url):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html5lib')\n",
        "    table = soup.find(class_ = \"infobox vcard\").tbody \n",
        "    tag = table.find(string=re.compile(\"Assumed office\")).find_parent('td').text\n",
        "    assuming_office = tag\n",
        "    return assuming_office\n",
        "\n",
        "#getting the president political party\n",
        "def get_president_political_party(url):  #need to pass the url of the president here\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content)\n",
        "    table = soup.find(class_ = \"infobox vcard\").tbody.children     \n",
        "    if type(table)!= bs4.element.NavigableString:           \n",
        "        for table_data in table:\n",
        "            if type(table_data)!= bs4.element.NavigableString:\n",
        "                tag = table_data.find(\"th\")\n",
        "                if (tag is None):\n",
        "                  continue\n",
        "                if (tag.text == \"Political party\"):\n",
        "                  political_party = tag.find_next_sibling().text \n",
        "    return political_party \n",
        "\n",
        "##########################################################################\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/Abdelmadjid_Tebboune\"\n",
        "url = \"https://en.wikipedia.org/wiki/Abdel_Fattah_el-Sisi\"\n",
        "name = get_president_name(url)\n",
        "birthdate = get_president_birthdate(url)\n",
        "assumed = get_president_assuming_office_date(url)\n",
        "party = get_president_political_party(url)\n",
        "print(name)\n",
        "print(birthdate)\n",
        "print(assumed)\n",
        "print(party)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jo%C3%A3o Louren%C3%A7o\n",
            " (1954-03-05) \n",
            "Assumed office 26 September 2017\n",
            "People's Movement for the Liberation of Angola (MPLA) (1974â€“present)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMA6d-8VxDqq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVM2E4mnxDnK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qylOhtHexC60"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdk5ZvKFxC5I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogfhIiS_xC28"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjOlzxrFxC0o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Nc8CqAxCyb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSCutYY2xCsT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-BdTE1QOiuT",
        "outputId": "1f902a89-ff1b-49d2-9028-b6802fcbb028"
      },
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Abdelmadjid_Tebboune\"\n",
        "x=url.rsplit('/',1)\n",
        "x=url.rsplit('/',1)[1]\n",
        "x=x.replace('_', ' ')\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abdelmadjid Tebboune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFlLuu7QOh77",
        "outputId": "a9ed5475-2aa5-4182-b22c-74c1ab5b8977"
      },
      "source": [
        "#trial cell\n",
        "presidents['political_party'] = []\n",
        "page = requests.get(\"https://en.wikipedia.org/wiki/Abdelmadjid_Tebboune\")\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "# table = soup.find(class_ = \"infobox vcard\").tbody \n",
        "# tag = table.find(string=re.compile(\"Assumed office\")).find_parent('td').text\n",
        "# print (tag)\n",
        "table = soup.find(class_ = \"infobox vcard\").tbody.children     \n",
        "if type(table)!= bs4.element.NavigableString:           \n",
        "    for table_data in table:\n",
        "        if type(table_data)!= bs4.element.NavigableString:\n",
        "            tag = table_data.find(\"th\")\n",
        "            if (tag is None):\n",
        "              continue\n",
        "            if (tag.text == \"Born\"):\n",
        "              birthdate = tag.find_next_sibling()\n",
        "              birthdate = birthdate.find(\"span\").text\n",
        "              print(birthdate)\n",
        "                "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (1945-11-17) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHYTrQhNkN6T"
      },
      "source": [
        "############### defining the methods to get country info ###############\n",
        "#getting the area of the country\n",
        "def get_country_area()\n",
        "return \n",
        "\n",
        "#getting the area of the country\n",
        "def get_country_area()\n",
        "return \n",
        "\n",
        "#getting the area of the country\n",
        "def get_country_area()\n",
        "return \n",
        "\n",
        "#getting the area of the country\n",
        "def get_country_area()\n",
        "return \n",
        "\n",
        "########################################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbTZrHn0kN2R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERQ77jXWkN0S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv47n35IkNyB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR6yFUWjkNqi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKqumWElvTQS"
      },
      "source": [
        "page = requests.get(\"https://en.wikipedia.org/wiki/List_of_sovereign_states_and_dependent_territories_in_Africa\")\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "# getting the table rows\n",
        "table_rows = list(soup.find(class_ = \"wikitable sortable\").tbody.children) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M5PZrd4C0Vb"
      },
      "source": [
        "table_rows_num = len(table_rows)\n",
        "country_name = None     \n",
        "capital_name = None\n",
        "latitude = None\n",
        "longitude = None\n",
        "Population = None\n",
        "language = None\n",
        "counter = 0; \n",
        "\n",
        "for i in range(table_rows_num): \n",
        "    table_row = table_rows[i]\n",
        "    country_name = None\n",
        "    capital_name = None\n",
        "    language = None\n",
        "    if type(table_row)!= bs4.element.NavigableString:           \n",
        "        for table_data in table_row.children:\n",
        "            # getting country_name\n",
        "            if type(table_data)!= bs4.element.NavigableString:\n",
        "                country_tag = table_data.find(\"a\", title=True)\n",
        "                if country_tag:\n",
        "                    if not country_name:\n",
        "                        country_name = country_tag.text\n",
        "                        counter = counter + 1 \n",
        "                        print(country_name)\n",
        "            \n",
        "            # getting capital_name\n",
        "            #if type(table_data)!= bs4.element.NavigableString:\n",
        "            #    capital_tag = table_data.find(\"a\", title=True)\n",
        "            #    if capital_tag:\n",
        "            #        if not capital_name:\n",
        "            #            capital_name = capital_tag.text\n",
        "            #            print(capital_name)\n",
        "\n",
        "print(\"Found\", counter, \"countries in Africa\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97b86fee"
      },
      "source": [
        "import re\n",
        "def make_sure_with_regex_that_this_is_ib_country_largest(class_attribute_value):\n",
        "    return class_attribute_value and re.compile(\"ib-.*-largest\").search(class_attribute_value)\n",
        "\n",
        "table_rows_num = len(table_rows) # to define the range of our loop\n",
        "# we will store the scraped data here\n",
        "country_name = None     \n",
        "capital_name = None\n",
        "latitude = None\n",
        "longitude = None\n",
        "\n",
        "# of course you can consume the iterator like `for row in table_rows:`\n",
        "# but I found it helpful to manipulate the current index of iteration in some situations\n",
        "\n",
        "for i in range(table_rows_num): \n",
        "    table_row = table_rows[i]\n",
        "    for table_data in table_row.children:\n",
        "        \n",
        "        # getting name\n",
        "        country_name_tag = table_data.find(class_=\"country-name\")\n",
        "        \n",
        "        # this condition will only be true if the current table_row\n",
        "        # contains \"country_name\" class value ... this will make it\n",
        "        # safe to run the \"parsing\" logic for every row without\n",
        "        # the fear of overwriting the data you scraped\n",
        "        if country_name_tag:\n",
        "            country_name = country_name_tag.text\n",
        "        \n",
        "        # getting the capital name and coordinates\n",
        "        capital_tag = table_data.find(class_=make_sure_with_regex_that_this_is_ib_country_largest)\n",
        "        # See the if condition again so we don't overwrite the data\n",
        "        # we scraped\n",
        "        if capital_tag:\n",
        "            second_child = list(table_row.children)[1]\n",
        "            capital_name_tag = second_child.find(title=True)\n",
        "            if capital_name_tag:\n",
        "                capital_name = capital_name_tag.text\n",
        "\n",
        "            latitude = second_child.find(class_=\"latitude\").text\n",
        "            longitude = second_child.find(class_=\"longitude\").text        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNSXF2x3zr5Q",
        "outputId": "6d1f4d23-b895-4c7a-8d2e-98afa8aa6858"
      },
      "source": [
        "#print the elements separated by ..\n",
        "print(country_name, capital_name, latitude, longitude, sep = ' .. ')\n",
        "\n",
        "# we only have one row of data \n",
        "import pandas as pd\n",
        "to_be_transformed_into_pandas_dataframe = [{\"id\": 1, \"name\": country_name, \"capital\": capital_name, \"latitude\": latitude, \"longitude\": longitude}]\n",
        "# transform into dataframe\n",
        "df = pd.DataFrame(to_be_transformed_into_pandas_dataframe)\n",
        "\n",
        "#print the data frame\n",
        "df\n",
        "\n",
        "#Load this dataframe into csv that would finally go into your database\n",
        "df.to_csv(\"country_data.csv\", sep=',', encoding='utf-8', index=False)\n",
        "\n",
        "#try reading the new file\n",
        "!cat country_data.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hellenic Republic .. Athens .. 37Â°58â€²N .. 23Â°43â€²E\n",
            "id,name,capital,latitude,longitude\n",
            "1,Hellenic Republic,Athens,37Â°58â€²N,23Â°43â€²E\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EuWkyxW0nfT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD1A4_8YW3KA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-A9QJbsW3Hy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCFX-Xe8W3G3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haqtJxmEW3Cn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdbeRZ5gW298"
      },
      "source": [
        "############################ scratches #################################\n",
        "#print the list of all continents names and their wiki URLs\n",
        "print( continents ['continent_name'])\n",
        "print(*continents ['continent_url'], sep = \"\\n\")\n",
        "\n",
        "#print the countries\n",
        "print(\"number of countries of the world = \", countries_counter)\n",
        "print (countries['country_name'])\n",
        "print (countries['country_index'])\n",
        "print (countries['country_url'])\n",
        "\n",
        "\n",
        "\n",
        "############################ scratches #################################\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}